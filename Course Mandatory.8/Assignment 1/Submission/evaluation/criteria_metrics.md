# Evaluation Criteria for Prompt Effectiveness

## Introduction

I've developed a comprehensive set of evaluation criteria to systematically assess the effectiveness of my prompts for the virtual project management consultant. These criteria enable objective measurement of response quality and guide my refinement process.

## Core Evaluation Dimensions

The evaluation framework consists of six core dimensions, each with specific criteria and measurement approaches:

### 1. Relevance & Specificity (25%)

**Key Question**: How well does the response address the specific project management scenario and context provided?

**Criteria**:
- **Contextual Adaptation**: Degree to which the response incorporates and adapts to the context parameters provided (project type, team composition, methodology, etc.)
- **Domain Specificity**: Use of domain-appropriate language, concepts, and examples relevant to project management
- **Scenario Alignment**: Focus on addressing the specific scenario rather than generic project management advice
- **Parameter Responsiveness**: Variation in response based on changes to context parameters

**Measurement Approach**:
- 5-point scale for each criterion (1=Poor, 5=Excellent)
- Identify specific instances of contextual elements being incorporated
- Compare responses across different parameter settings

### 2. Actionability & Practicality (25%)

**Key Question**: How immediately useful and implementable is the advice provided?

**Criteria**:
- **Implementation Guidance**: Clear steps for putting advice into practice
- **Resource Consideration**: Acknowledgment of realistic resource constraints
- **Immediate Applicability**: Inclusion of actions that can be taken immediately
- **Tool & Template Provision**: Specific tools, templates, or frameworks provided

**Measurement Approach**:
- Count of actionable steps provided
- Assessment of resource realism (high/medium/low)
- Identification of immediate vs. long-term actions
- Count and quality of tools/templates included

### 3. Comprehensiveness & Structure (20%)

**Key Question**: Does the response cover all requested elements in a well-organized manner?

**Criteria**:
- **Component Coverage**: Inclusion of all requested components in the prompt
- **Organizational Clarity**: Logical flow and clear structure using headings, lists, etc.
- **Depth Balance**: Appropriate allocation of depth to different components based on importance
- **Information Hierarchy**: Effective prioritization of information from most to least important

**Measurement Approach**:
- Checklist of requested components (all/most/some/few)
- Assessment of organizational elements (headings, sections, lists)
- Proportion of content devoted to each component

### 4. Expertise & Credibility (15%)

**Key Question**: Does the response demonstrate project management expertise and provide credible guidance?

**Criteria**:
- **Best Practice Alignment**: Consistency with established project management best practices
- **Methodology Accuracy**: Correct representation of methodologies mentioned
- **Reasoning Transparency**: Clear rationale provided for recommendations
- **Authority Markers**: Inclusion of elements that demonstrate expertise (statistics, research references, experience-based insights)

**Measurement Approach**:
- Expert review of best practice alignment (high/medium/low)
- Identification of methodology misrepresentations
- Count of explained rationales vs. unsupported assertions
- Presence of authoritative elements

### 5. Adaptability & Flexibility (10%)

**Key Question**: How well does the response acknowledge variation and provide adaptable guidance?

**Criteria**:
- **Contingency Consideration**: Acknowledgment of different scenarios or outcomes
- **Scalability Guidance**: Advice on how to scale approaches for different project sizes
- **Methodology Flexibility**: Adaptation of advice for different project methodologies
- **Constraint Adaptability**: Options for adapting advice under different constraints

**Measurement Approach**:
- Count of contingency scenarios addressed
- Presence of scaling guidance (yes/limited/no)
- Assessment of methodology-specific variations
- Range of constraint adaptations provided

### 6. Engagement & Clarity (5%)

**Key Question**: How clear, engaging, and usable is the response?

**Criteria**:
- **Language Clarity**: Use of clear, jargon-appropriate language
- **Engagement Elements**: Inclusion of engaging elements (examples, analogies, visualizations)
- **Instructional Quality**: Effectiveness of explanations and instructions
- **Cognitive Accessibility**: Appropriate complexity level for the intended audience

**Measurement Approach**:
- Readability metrics (Flesch-Kincaid)
- Count of engaging elements
- User comprehension assessment
- Complexity analysis (sentence length, technical term frequency)

## Scoring System

Each prompt response is evaluated using a combination of:

1. **Criterion Scores**: 1-5 rating for applicable criteria (1=Poor, 5=Excellent)
2. **Dimension Scores**: Average of criterion scores within each dimension
3. **Overall Effectiveness Score**: Weighted average of dimension scores
4. **Qualitative Assessment**: Narrative evaluation of strengths and weaknesses

## Evaluation Scorecard Template

| Dimension | Criteria | Score (1-5) | Notes |
|-----------|----------|-------------|-------|
| **Relevance & Specificity (25%)** | Contextual Adaptation |  |  |
|  | Domain Specificity |  |  |
|  | Scenario Alignment |  |  |
|  | Parameter Responsiveness |  |  |
| **Actionability & Practicality (25%)** | Implementation Guidance |  |  |
|  | Resource Consideration |  |  |
|  | Immediate Applicability |  |  |
|  | Tool & Template Provision |  |  |
| **Comprehensiveness & Structure (20%)** | Component Coverage |  |  |
|  | Organizational Clarity |  |  |
|  | Depth Balance |  |  |
|  | Information Hierarchy |  |  |
| **Expertise & Credibility (15%)** | Best Practice Alignment |  |  |
|  | Methodology Accuracy |  |  |
|  | Reasoning Transparency |  |  |
|  | Authority Markers |  |  |
| **Adaptability & Flexibility (10%)** | Contingency Consideration |  |  |
|  | Scalability Guidance |  |  |
|  | Methodology Flexibility |  |  |
|  | Constraint Adaptability |  |  |
| **Engagement & Clarity (5%)** | Language Clarity |  |  |
|  | Engagement Elements |  |  |
|  | Instructional Quality |  |  |
|  | Cognitive Accessibility |  |  |
| **Overall Score** |  |  |  |

## Success Thresholds

The following thresholds determine the success of a prompt:

- **Excellent**: Overall score ≥ 4.5, no dimension below 4.0
- **Good**: Overall score ≥ 4.0, no dimension below 3.5
- **Acceptable**: Overall score ≥ 3.5, no dimension below 3.0
- **Needs Improvement**: Overall score < 3.5 or any dimension below 3.0

## Continuous Improvement Framework

The evaluation results feed into a continuous improvement process:

1. **Gap Analysis**: Identifying dimensions with the lowest scores
2. **Root Cause Analysis**: Determining why certain criteria are not being met
3. **Targeted Refinement**: Modifying prompts to address specific weaknesses
4. **Iteration Testing**: Re-testing refined prompts to measure improvement
5. **Cross-Prompt Learning**: Applying successful elements from high-scoring prompts to others

## Usage in This Project

I've applied this evaluation framework throughout the development process:

1. **Initial Prompt Assessment**: Evaluating first drafts to identify major gaps
2. **Comparative Testing**: Measuring improvements between iterations
3. **User Feedback Validation**: Correlating user feedback with objective metrics
4. **Final Verification**: Ensuring all prompts meet or exceed the "Good" threshold

This systematic approach has enabled me to progressively improve the effectiveness of the prompts, resulting in a final set that consistently produces high-quality, actionable project management advice.